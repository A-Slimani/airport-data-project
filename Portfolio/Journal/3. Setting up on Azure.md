Since I want to gain data engineering experience on the Azure platform I have decided to upload the json files onto Azure blob storage. I will be using Gen 2 Datalake which is the recommended option for data engineering projects with azure. This will allow me to have a hierarchical structure in the blob storage compared to the standard blob storage option.

I also moved over the scraper to use Azure functions. This will now mean I do not have to run the scraper manually everyday. I also setup the scraper to automatically upload to the blob storage RAW folder.

For now I have scheduled for the scraper to run daily at 1 AM. This will ensure I get the previous days data once that day has been complete at an early time. It will also limit potentially having to manually rerun the scraper to update missing data. However I am not totally stuck on this and will change depending on each airport API and its quirks. 