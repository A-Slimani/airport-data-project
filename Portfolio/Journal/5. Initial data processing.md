Right now I have the data I need to start the transformation step of the pipeline. I have decided to use dbt for transformation as I prefer using SQL to create table transformations. I will also use DuckDB as it is faster for aggregations and I don't have to host the DB as a service like in PostgreSQL.  

My initial option was to be totally on the Azure platform by using synapse however it is currently in maintenance mode which makes it unappealing to start and I cannot use fabric since its on a different platform where I need a Microsoft business account. My best option is to run the dbt + DuckDB transformation inside either an Azure function or an Azure instance. 

## Setting up DuckDB + dbt to connect to Azure DataLake

### Local
I will first setup on my local machine to run and test the scripts that I will develop for transformation. The initial setup was easy just had to add dbt and the DuckDB adapter to the current python project. After adding that I had to configure the profile which I faced some difficulty. 

For some reason when running DuckDB with azure it could not find CA certificate. I kept getting the error 

```log
Problem with the SSL CA cert (path? access rights?)
```

I spent different variety of solutions such as running the [Azure Extension](https://duckdb.org/docs/stable/core_extensions/azure) inside `dbt_profile.yml` or using different ways of accessing the datalake e.g. SAS Key, AZ login and even reinstalling the ca-certificate with `sudo apt install ca-certificates`. The solution was to include this inside the `profiles.yml` file. As to why this is the case I am unsure I think leaving to default is replacing the ca-certificate to something else.

```yml
settings:
	azure_transport_option_type: 'curl'
```

Inside the dbt model I query the data using the query below. With this I can access all the JSON files as a single table. Right now the query takes around 9 seconds on a local machine using 2 threads on 38MB~ of data which isn't the biggest however I think its fine for now, potentially I could just split the query to smaller chunks or only process newer ones to save time.


```sql
SELECT * FROM read_json_auto('.../path/to/dir/*.json')
```


## Adding Silver layer for processing

Here is where I will clean up my data. Leaning from previous experience I will try to add testing to provide an outline of what the data should look like and make transformations based off that. Adding deeper layers of testing will come later when I have more concrete ideas of what the data should look like.

So far this the one of tests that I have included, this is to ensure strings are consistent all round

```
```
```yml
- name: terminal
  description: "domestic or international"
  tests:
    - not_null
    - accepted_values:
        arguments:
          values: ['domestic', 'international']
```


When going through the current Sydney Airport dataset since its from a JSON file alot of "NULL" values are just empty strings (''). This means that the current not_null check is not valid unless I include `NULLIF()` checks. Also through further investigation I found that there were missing airline names however the records have the airline code. To resolve this I had to search the airline name based of the code. I then created a seed in dbt so that it will patch the missing airline names with the correct airline name based on the airline code

```csv
airline_code, airline_name
QN, SmartLynx Australia
SAN, Santa?
FI, Iceland Air
AXY, Air Charter?
VJ, VietJet
NP, North Pole?
SC, Santa Claus?
```

These were the current missing airline names. 3 of 7 where just Christmas easter eggs of which I will ignore. AXY seems like a private air charter which is interesting for it to show on the website also will not be included. Iceland Air is interesting there are generally no flights from Sydney using it however digging with a little research it seems that this is part of a [flight tour](https://gemini.google.com/share/aea98751e454). VietJet and SmartLynx seem to be the only "normal" consistent flights that have missing airline names. Current dbt seed will include legitimate flights and will filter out the other flights.

Here is my current fix
```SQL
...
  CASE
    WHEN flight_data.airline='' THEN m.airline_name 
    ELSE flight_data.airline
  END AS airline_name
...
```
