Since I have found an API endpoint from which I can scrape I will now write a scraper to download the json responses from the API which I want to be uploaded into a cloud storage. This was simple I just wrote a basic script used the python requests library to get the data from the api endpoint and downloading the data locally as well as uploading it to GCP cloud storage.

-- Include github link here for the scrapers --


## Adding new airports

Since I am doing this project alongside the [data engineering zoomcamp](https://datatalks.club/blog/data-engineering-zoomcamp.html) course there is some time before next week where it will start with workflow orchestration. Because of this I will try to add all the major Australian airports into my scraper to have more data to work with and develop skills in converging different data formats / layouts into one master database. 

So far I have added
- Melbourne Airport
- Brisbane Airport

Adding these were easy since both websites had APIs that I could call from. The difference between each of them were the different parameters that were needed to pass through. 


### Adding Adelaide Airport

Currently cant use Adelaide airport's API endpoint because it requires JavaScript enabled to be able to continue. The error message seen below.

```
Enable JavaScript and cookies to continue
```

I used the [[curl-cffi]] library to bypass this mimicking a real browser to bypass anti bot / scraping checks.

### Adding Perth Airport

The Perth API endpoint was a bit more complex compared to the previous ones. The API endpoint uses the same URL as the website itself which led to my scraper constantly returning a html page leaving me confused as to why I can access it in the browser and why its the same URL. 

After a while I found out that it was getting the json through a `POST` method. It also required to get a `requestVerificationToken` which is hidden in the website. To solve this I had to first load the home page get the token then make the `POST` call with the token included to return the API data. 

Additionally they only have data for the current day so I will have to adjust to this probably by scraping the data at the end of the day close to midnight.