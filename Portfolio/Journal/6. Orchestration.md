Now that I have my scraper scripts and initial data processing complete I want to start to develop the orchestration for the project. Trying to keep this project azure native I have decided to test out using Azure data factory to run my scrapers and then process the data with dbt which will also be triggered through an Azure function inside the pipeline.

Adding the function to the pipeline was simple however when running it I encountered some complications. When trying to debug it would return this vague error message.

```json
{
  "code": "BadRequest",
  "message": null,
  "target": "pipeline//runid/3d1e734f-7154-454a-b192-6c6ed41f6437",
  "details": null,
  "error": null
} 
```

My first thought was to check the azure function to see if it was being triggered which it was not leading me to believe that there was a problem in the URL. I decided instead of using the Azure Function I will just trigger it with the `Web Activity` option. It is less integrated than Azure functions however it works and provides the http response as well which I can use for pipeline orchestration.

== SCREENSHOT_HERE ==

I also added scheduling so that it would run 4am every morning. I will check the following day to ensure that it runs if not just run it manually and make the necessary fixes for it to work. 

== SCREENSHOT_HERE == 

I will now move on to build the Azure Function for dbt so that I can add it to the pipeline.


## Resolving Date mismatch issue

- Currently have a date mismatch issue with my scraper
  - Not scraping previous day rather scraping from 2 days before 
- Looking at the logs I see its using GMT standard time

```logs
Response status: 200 Response headers: 'Content-Type': 'application/json; charset=utf-8' 'Date': 'Tue, 03 Feb 2026 17:00:09 GMT' 'Server': 'Kestrel' 'Transfer-Encoding': 'chunked' 'X-CORRELATION-ID': 'REDACTED'
```

- should be 4:00 AM next day
- decide to just manually change the pipeline to run at 6PM UTC = 5 AM AEDT


## Resolving null datasets
I had an issue where I was getting missing datasets for my Melbourne and Perth scraper. The main issue that I found that it was still returning success message so to resolve this I had to modify the current scraper to raise an exception if the JSON was missing

```python
if data:
    return data
else:
    raise ValueError("Response returned valid but empty JSON")
```

I also had to change my try catch block to return an Exception rather than print out what the error was
```python
# OLD 
print(f"An error occurred when accessing the URL: {e}")
# NEW
raise Exception(f"An error occurred when accessing the URL: {e}")
```

With the Melbourne scraper for some reason the response JSON was not returned in the function. Unsure how that happened but added it in now and it works. For the Perth scraper there was a date issue it was trying to use the date library which was not imported. Updated it to use a valid date function so that it can make the HTTP call.

For further validation I can potentially use `jsonschema` library to validate the schema of the response. However I think for now this is sufficient and this is something I can potentially come back to later to further improve the data validation. 
